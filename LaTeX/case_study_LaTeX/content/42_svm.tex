\newpage
\section{Support Vector Machine (SVM)}

% - One to two sentences about how the model works
% - Why it was chosen by us (KNN Baseline)
% - Besonderheiten / Abweichungen von standard vorgehen.
% - (Vorgehensweise) & Results (be cautios to not repeat yourself from the 40_data_mining part)
% //Results = include Table with scores 
% // Hyperparameter tuning : Scoring = F1-Macro


\begin{table}[htbp]
    \centering
    \begin{tabular}{
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |}
    \cline{2-7}
    \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}}                     & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Precision}                                                 & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Recall}                                                    & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}F1-Makro}                                                  \\ \hline
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Adaption}            & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}none}                & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.497} & 0.510                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.393} & 0.387                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.396} & 0.387                                              \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 1. Hp Tuning}      & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.881} & 0.563                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.638} & 0.493                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.712} & 0.516                                              \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Balancing}         & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.893} & 0.529                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.890} & 0.671                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.887} & 0.557                                              \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Feature Selection} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.861} & 0.515                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.859} & 0.655                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.855} & 0.537                                              \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 2. Hp Tuning}      & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.952} & 0.537                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.951} & 0.618                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.950} & 0.562                                              \\ \hline
\end{tabular}
\caption{SVM - Predictions}
\label{tab:svm_pred}
\end{table}


Support vector machines (SVM) are computer algorithms which belong to the supervised learning methods. They learn by example to assign labels to objects and can therefore be especially useful for classification problems. To achieve this purpose, they comprise four basic concepts: the separating hyperplane, the maximum-margin hyperplane, the soft margin, and the kernel fuction. In short, SVMs draw an optimal hyperplane inbetween all of the classes that are distinguished and aim to maximize the distance between the data points and the hyperplane (maximum margin). \citep{Noble2006}
SVMs were chosen for this project because they are versatile and memory-efficient. Furthermore, they are particulary efficient for high-dimensonal spaces and classification problems in machine learning. \citep{ScikitLearn2021}
For SVMs, the general procedure mentioned in chapter \ref{sec:data_mining} was followed. After the preprocessing of the data, the initial model was created, trained and evaluated by using a SVM. The performance of the initial model was fairly low with the $F_1-Score$ barely reaching 40 \% for both train and test data. As a next step, the first hyperparameter tuning of the attributes 'C', 'gamma', and 'kernel' resulted in an improved model with an $F_1-Score$ of 71.2 \% for the train data and 51.6 \% for the test data. However, it became clear that this state of the model included a high bias towards class one (wine of medium quality) due to the unbalanced train and test dataset. 
The following model was based on a balanced dataset by using SMOTE to oversample the minority classes. The $F_1-Score$ increased by roughly 4 \% and the model was kept to solve the bias issue mentioned above. Although computing the feature importance was not possible due to the rbf kernel, other feature selecting techniques proved as non-viable as the $F_1-Score$ decreased. As a result, this model was excluded from the second hyperparameter tuning. 
Lastly, the second hyperparameter tuning resulted in the same optimal parameters and $F_1-Score$ as before in the oversampling step. Additional experimentation with the hyperparameters led to the final model with an $F_1-Score$ of 95 \% for the train data and 56.2 \% for the test data. In theory, it is possible to further increase the $F_1-Score$ for the test data to values above 60 \% by increasing the hyperparameters 'C' and 'gamma'. However, this would lead to an even higher overfit-tendency of the model. Therefore, this step was avoided. All results regarding the model evaluation can be found in table \ref{tab:svm_pred}.


