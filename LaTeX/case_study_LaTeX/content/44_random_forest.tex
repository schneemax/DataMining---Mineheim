\section{Random Forest}
\begin{table}[htbp]
    \centering
    \begin{tabular}{
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |}
    \cline{2-7}
    \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}}                            & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Precision}                                                 & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Recall}                                                    & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}F1-Makro}                                                  \\ \hline
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Adaption}                   & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}none}                       & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.878                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.552                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.596                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 1. Hp Tuning} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.962} & 0.525                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.622} & 0.476                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.666} & 0.489                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Balancing}                & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.966} & 0.579                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.966} & 0.675                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.965} & 0.611                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Feature Selection}        & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.967} & 0.558                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.967} & 0.644                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.966} & 0.585                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 2. Hp Tuning} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.966} & 0.577                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.965} & 0.667                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.965} & 0.607                                             \\ \hline
\end{tabular}
    \caption{Random Forest - Predictions}
    \label{tab:rf_pred}
\end{table}

The random forest algorithm is an extension of the decision tree which is defined in \ref{sec:decision_tree}. Developed from the statistician Leon Breimann, the random forest is a combination of different decision trees: \enquote{A random forest is a classifier consisting of a collection of tree-structured classifiers $\{h(x,\Theta_k), k = 1, ...\}$ where the $\{\Theta_k\}$ are independent identically distributed random vectors and each tree casts a unit vote for the most popular class at input $x$.}\citep[][p. 6]{Breiman2001}

The first model based on the random forest algorithm is created with the default parameters of the random forest. This creation results in an overfitted model which has a precision, recall and $F_1-Score$ of 1 within the training and a precision, recall and $F_1-Score$ of between 0.552 and 0.878 in the testing. In addition, it can be depictd that the trained model by default does not make mistakes by the predicition. The tested model only predicts the wine of a middle quality. This might be the result of an unbalanced data set where the most entries are classified in the middle class and only a few wines are classified as bad or good. The parameter of the initially created model will be hypertuned. These parameters will be used for the next to be created models. The hypertuned parameters e.g., include a maximal depth of 13 of the tree and 700 tress in the random forest.\citep{Hoffman2020} It can be depicted that the model with the hypertuned parameter does not reveal any major overfitting issues anymore as seen in the table \ref{tab:rf_pred} However, the tested model does not classify any wine as class 0 (\enquote{bad}).

Consequently, the second model will include the methode oversampling in order to balance the data set. This modification results in an improved but overfitted model. The precision, recall and $F_1-Score$ of the training differs between 0.965 and 0.966 whereas the precision, recall and $F_1-Score$ of the testing stretches over the interval 0.579 and 0.675. Besides the remaining overfitting issue, the trained and tested model predict every class.

In order to tackle the remaining overfitting issue, the feature importance, permuation importance as well as feature correlation is analyzed.\citep{Piotri2020} It can be depicted that dropping columns depending on the results of the feature importance, permutation importance or feature selection and the combination of the three, does not reveal any enhanced model.Therefore, no feature selection is applied in the next step. In table \ref{tab:rf_pred} the fourth row displays the best option of the three used techniques which is the feature importance.
% It can be depicted that the feature type is of low importance for the model. Therefore, this feature is deleted. The results show, that the exclusion of the feature type does not improve the model significantly. According to the permutation importance the features pH, total sulfur dioxide, citric acid, and density must be deleted which does not reveal an enhanced model either. The feature correlation aims to delete the columns fixed acidity, alcohol, type, residual sugar, and density. As this model does not improve the model, all three methods of feature selection are combined, deleting those features, which are deleted by at least two methods. As a result, the attributes type and density are removed. As this final exclusion does not reveal an improvement according to the metric $F1_Score$, the feature selection is not applied in the further steps.

Lastly, a second hyperparmeter tuning is conducted in order to adapt the hyperparameters to the newly created model including the oversampling. This second hyperparameter tuning results in a slightly improved model which can be seen in table \ref{tab:rf_pred}.




