\newpage
\section{Random Forest}
\begin{table}[htbp]
    \centering
    \begin{tabular}{
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |}
    \cline{2-7}
    \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}}                            & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Precision}                                                 & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Recall}                                                    & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}F$_1$-Makro}                                                  \\ \hline
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Adaption}                   & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}none}                       & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.878                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.552                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.596                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 1. Hp Tuning} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.962} & 0.525                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.622} & 0.476                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.666} & 0.489                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Balancing}                & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.966} & 0.579                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.966} & 0.675                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.965} & 0.611                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Feature Selection}        & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.967} & 0.558                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.967} & 0.644                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.966} & 0.585                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 2. Hp Tuning} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.966} & 0.577                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.965} & 0.667                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.965} & 0.607                                             \\ \hline
\end{tabular}
    \caption{Random Forest - Predictions}
    \label{tab:rf_pred}
\end{table}

The random forest algorithm, developed by the statistician Leon Breimann, is in general a combination of different decision trees. \citep[][p. 6]{Breiman2001}
The first model based on the random forest algorithm was created with the default parameters of the random forest. This creation resulted in an overfitted model, which had a precision, recall and F$_1$-Score of 100 \% within the training and a precision, recall and F$_1$-Score of between 55.2 \% and 87.8 \% in the testing. Therefore, the trained model did not make any mistakes. However, the tested model only predicted the wine of a medium quality. This might be the result of an unbalanced dataset where most entries are classified in the medium class and only a few wines are classified in the lower or higher class. The parameters of the initially created model were hypertuned (e.g, maximal depth of 13 of the tree) and were used for the other models. \citep{Hoffman2020} It can be depicted that the model with the hypertuned parameter did not reveal any major overfitting issues anymore as seen in the table \ref{tab:rf_pred}. However, the tested model did not classify any wine as class 0 (\enquote{bad}).

Consequently, the second model included an oversampling method in order to balance the dataset. This modification resulted in an improved but overfitted model. The precision, recall and F$_1$-Score of the training differed between 96.5 \% and 96.6 \% whereas the precision, recall and F$_1$-Score of the testing stretches over the interval of 57.9 \% and 67.5 \%. Besides the remaining overfitting issue, the trained and tested model predicted every class.

In order to tackle the remaining overfitting issue, the feature importance, permutation importance and feature correlation were analyzed. \citep{Piotri2020} It can be depicted that dropping columns depending on the results of the \textit{feature importance}, \textit{permutation importance} or \textit{feature selection} and the combination of the three, did not reveal any enhanced model. Therefore, no feature selection is applied in the next step. In table \ref{tab:rf_pred} the fourth row displays the best option of the three used techniques which is the feature importance.
% It can be depicted that the feature type is of low importance for the model. Therefore, this feature is deleted. The results show, that the exclusion of the feature type does not improve the model significantly. According to the permutation importance the features pH, total sulfur dioxide, citric acid, and density must be deleted which does not reveal an enhanced model either. The feature correlation aims to delete the columns fixed acidity, alcohol, type, residual sugar, and density. As this model does not improve the model, all three methods of feature selection are combined, deleting those features, which are deleted by at least two methods. As a result, the attributes type and density are removed. As this final exclusion does not reveal an improvement according to the metric $F1_Score$, the feature selection is not applied in the further steps.

Lastly, a second hyperparameter tuning was conducted to adapt the hyperparameters to the newly created model including the oversampling. This second hyperparameter tuning resulted in a slightly improved model.






