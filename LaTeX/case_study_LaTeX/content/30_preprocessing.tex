\chapter{Preprocessing}\label{chap:preprocessing}
Before the models for predicting the quality of a wine can be created, the data has to be prepared accordingly. Preprocessing is important in order to \enquote{resolve several types of problems include noisy data, redundancy data, missing data values, etc.}\cite[p. 116]{Kotsiantis2006}. The preprocessing of the utilized dataset for the characteristics as well as the qualita of wine can be divided into three parts.

First of all, duplicates need to be deleted and missing values must be handled in an appropiate way. The data set of 6497 entries in the initial state does not include any duplicates. Consequently, no rows must be deleted. As analyzed in the section \ref{sec:data_structure} the data set has 34 missing values or so-called null values in seven different column. As a result of the low number of null values, they are deleted without the application of a missing data handling.

In the second step the columns are preprocessed. Numerical values are getting normalized and categorical values are converted into numerical values. According to Kotsiantis et al. normalization is a \enquote{\enquote{scaling down} transformation of the features.}\cite[p. 113]{Kotsiantis2006} Therefore, the all numerical features as for instance fixed acidity, volatile acidity, density, pH, and alcohol. The categorical value type, which determines whether the wine is red or white, is encoded to numerical values with the help of the \textit{Ordinal Encoder}.\cite{OrdinalEncoder2021} As the target variable quality has been adapted in the section \ref{sec:data_structure}, no binning has to be applied. No additional features were genererated, as the data set mainly contains of chemical parameters as e.g., the pH or sulphates, which cannot be combined to build new features. The feature selection and correlation is part of the adaption phase of the models as the feature selection differs between the applied algorithms.

Thirdly, the outliers which have been detected in section \ref{sec:data_structure}, are removed.\cite{AskPython2021} The outliers are deleted on the basis of a 90 \% confidence interval. Therefore, every value that is not included in the 90 \% confidence interval is deleted. As a result of the outlier detection as well as deletion 57 are removed.

In the last step the feature and target variable are separated and stored in two different dataframes. Afterwards, the test and train split is applied with a test size of 20 \% of the available data and a stratification of the target variable.
% Preprocessing
% – Are missing values replaced (in case needed)?
% – Checked for outliers (and handled them)?
% – Validity tests of attributes (Height above sea level < 9000)?
% – Check for inconsistencies (age=42, birthday=03/07/1997)

% Check for duplicates
% – Data normalization
% – Additional features generated?
% – Has binning been tried out?
% – Correlation analysis implemented?
% – Feature subset selection implemented?
% • External Knowledge:
% – Are additional datasets used?

%// Single Source of truth preprocessing: /DataMining---Mineheim/Code/Random%20Forest/Wine_Quality_Prediction_rf_preventing_overfitting.ipynb