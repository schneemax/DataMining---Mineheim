\section{K-Nearest Neighbors (K-NN)}
\label{sec:knn}

% - One to two sentences about how the model works
% - Why it was chosen by us (KNN Baseline)
% - Besonderheiten / Abweischungen von standard vorgehen.
% - (Vorgehensweise) & Results (be cautios to not repeat yourself from the 40_data_mining part)
% //Results = include Table with scores 
% // Hyperparameter tuning : Scoring = F1-Macro

\begin{table}[htbp]
    \centering
    \begin{tabular}{
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |}
    \cline{2-7}
    \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}}                            & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Precision}                                                 & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Recall}                                                    & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}F1-Makro}                                                  \\ \hline
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Adaption}                   & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}none}                       & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.794} & 0.632                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.586} & 0.514                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.631} & 0.532                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 1. Hp Tuning} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.794} & 0.632                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.586} & 0.514                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.631} & 0.532                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Balancing}                & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.562                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.636                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.586                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Feature Selection}        & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.573                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.657                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.600                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 2. Hp Tuning} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.573                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.657                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.600                                             \\ \hline
\end{tabular}
\caption{KNN - Predictions}
\label{tab:knn_pred}
\end{table}

K-Nearest Neighbors (K-NN) is a rather simple algorithm, which is based on the distances between different data points to classify data. The foundamental idea of this algorithm is that the distance between data points that belong to the same class is less than to data points that are in a different class.\cite{Yildirim2020}

The initial training of the model, using the default values of the K-NN algorithm, resulted in rather unprecise measurements. The Precision and Recall values in table \ref{tab:knn_pred} show that only around half of the data points are classified correctly. Furthermore, the Precision values are slightly higher than the Recall values, which might indicate more false negative class predictions. Nonetheless, these results are a usable baseline. Afterwards a first hyperparameter tuning was executed, which resulted in almost the same exact default values. Meaning that the default values are already the most optimized combination of execution steps. Parameters that are tested through the hyperparameter tuning are mainly various settings for different calculation models for estimating the distance between data points. On top of that \textit{n\_neighbors} sets the number of relevant neighbor data points that are included into the calculation.

As a next step to increase the prediction of the model, the training data set was balanced. This is done through oversampling. The result of this balancing is that the predictions on the training data is a 100\% correct, whilste the prediction for the unbalanced test data is still rather poor. This indicates an overfitted model. Looking at the test data predictions, the Recall and F1-Makro could only be slightly increased and the Precision test data even got worse (table \ref{tab:knn_pred}). This might indicated more false positive predictions. In order to reduce the overfitting of the model a feature selection is used to make the data less noisy. This is done by calcualting the \textit{Permutation Importances} of the data sets features and by calculating the \textit{Feature Correlation}. The model is tested based on both feature selection methods and finally through a combination of both. This combination, which got the best output, still results in a overfitted model. However, Precision test, Recall test and the F1-Makro test score could be increased very slightly. This indicates that the feature selection was at least slightly benefitial for the model. A second hyperparameter tuning concludes that the default values for the model are again the best fit. The final scores are therefore not changed through the second hyperparameter tuning and the model is still overfitted.


