\newpage
\section{K-Nearest Neighbors (K-NN)}
\label{sec:knn}

% - One to two sentences about how the model works
% - Why it was chosen by us (KNN Baseline)
% - Besonderheiten / Abweischungen von standard vorgehen.
% - (Vorgehensweise) & Results (be cautios to not repeat yourself from the 40_data_mining part)
% //Results = include Table with scores 
% // Hyperparameter tuning : Scoring = F1-Macro

\begin{table}[htbp]
    \centering
    \begin{tabular}{
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |}
    \cline{2-7}
    \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}}                            & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Precision}                                                 & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Recall}                                                    & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}F$_1$-Makro}                                                  \\ \hline
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Adaption}                   & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}none}                       & \multicolumn{1}{l|}{\cellcolor[HTML] {FFFFFF}0.825} & 0.680                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.586} & 0.522                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.627} & 0.556                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 1. Hp Tuning} & \multicolumn{1}{l|}{\cellcolor[HTML]             {FFFFFF}0.825} & 0.680                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.586} & 0.522                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.627} & 0.556                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Balancing}                & \multicolumn{1}{l|}{\cellcolor[HTML] {FFFFFF}1.000} & 0.573                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.659                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.599                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Feature Selection}        & \multicolumn{1}{l|}{\cellcolor[HTML] {FFFFFF}1.000} & 0.558                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.642                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.585                                             \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 2. Hp Tuning} & \multicolumn{1}{l|}{\cellcolor[HTML]             {FFFFFF}1.000} & 0.558                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.642                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.585                                             \\ \hline
\end{tabular}
\caption{KNN - Predictions}
\label{tab:knn_pred}
\end{table}

K-Nearest Neighbors (K-NN) is a rather simple algorithm, which is based on the distances between different data points to classify data. \citep{Yildirim2020} %The foundamental idea of this algorithm is that the distance between data points that belong to the same class is less than to data points that do belong in a different class.\citep{Yildirim2020}
The initial training of the model, using the default values of the K-NN algorithm, resulted in rather unprecise measurement. The Precision and Recall values in table \ref{tab:knn_pred} show that only around half of the data points were classified correctly. %Furthermore, the Precision values were higher than the Recall values, which might indicate more false negative class predictions. 
Nonetheless, these results were a usable baseline. Afterwards a first hyperparameter tuning was executed, which resulted in almost the same exact default values. Meaning that the default values were already the most optimized combination of hyperparameters. Parameters that were tested through the hyperparameter tuning are mainly various settings for different calculation models for estimating the distance between data points. On top of that \textit{n\_neighbors} sets the number of relevant neighbor data points that are included in the prediction.
As a next step to improve the prediction of the model, the training data set was balanced through oversampling. After oversampling the result of the model increased to a 100 \% correct score on the training data, whilste the prediction for the test data was still rather poor. This indicates an overfitted model. Looking at the test data predictions in general, the Recall and F$_1$-Score could only be slightly increased and the Precision test data even got worse (table \ref{tab:knn_pred}). %This might indicated more false positive predictions. 
In order to reduce the overfitting of the model a feature selection was used to make the data less noisy. This was done by calcualting the \textit{Permutation Importances} of the data sets features and by calculating the \textit{Feature Correlation}. The model was tested based on both feature selection methods and finally through a combination of both. This combination, which got the best output, still resulted in an overfitted model. Furthermore, Precision test, Recall test and the F$_1$-Score test score performed slightly worse. %This indicates that the feature selection was at least slightly benefitial for the model. 
Finally, a second hyperparameter tuning concludes that the default hyperparameters for the model were again the best fit. The final scores were therefore not changed through the second hyperparameter tuning and the model was still overfitted. %The theoretically best scores, even though the model was still overfitted, could be archieved after balancing the data set and before any feature selection was executed.


