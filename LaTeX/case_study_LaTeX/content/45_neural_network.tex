\section{Neural Networks}\label{sec:neural_networks}

% - One to two sentences about how the model works
% - Why it was chosen by us (KNN Baseline)
% - Besonderheiten / Abweischungen von standard vorgehen.
% - (Vorgehensweise) & Results (be cautios to not repeat yourself from the 40_data_mining part)
% //Results = include Table with scores 
% // Hyperparameter tuning : Scoring = F1-Macro

\begin{table}[!h]
    \centering
    \begin{tabular}{
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |
    >{\columncolor[HTML]{FFFFFF}}l 
    >{\columncolor[HTML]{EFEFEF}}l |}
    \cline{1-7}
    \multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Preproc.}             & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Batch size}                            & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Layers}          & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Neurons}             & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Epochs}        & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}F$_1$ Val}           & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}F$_1$ Test}          \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{FFFFFF}Standard}     & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}64}                                            & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}3}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}100}                 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}30}                     & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.406}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.434}          \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{FFFFFF}+ Balancing}      & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}64}                                         & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}3}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}100}                 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}30}                      & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.707}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.456}          \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{FFFFFF}+ Balancing}       & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}64}                                        & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}4}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}300}                 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}30}                      & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.738}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.487}          \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{FFFFFF}+ Balancing}       & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}64}                                        & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}5}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}400}                 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}30}                      & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.761}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.544}          \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{FFFFFF}+ Balancing}       & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}16}                                        & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}7}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}1,000}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}40}                      & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.820}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.553}         \\ \hline
    \multicolumn{1}{|l|}{\cellcolor[HTML]{FFFFFF}+ Balancing}       & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}16}                                        & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}8}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}2,048}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}80}                      & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.851}               & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}0.567}         \\ \hline
\end{tabular}
\caption{Neural network -- Predictions}
\label{tab:nn_pred}
\end{table}

Neural networks (NNs) are supposed to emulate the learning process of the human brain by connecting artificial neurons and giving these connections a certain weight. Adjusting these weights is the \enquote{learning} part of the algorithm \citep[][p. 326]{Aggarwal2015}.

After the preprocessing steps that were explained in chapter \ref{chap:preprocessing}, the train dataset was split again to create a validation dataset, which was used during the \textit{training} or \textit{fitting} phase of the NN. It was used to -- as the name suggests -- validate the interim results of the network and adjust the hyperparameters \citep[][p. 184]{TanPang-Ning2006}.

Having split the data again, it had to be converted into tensors in order to be able to be used in the NN. This had to be done for all of datasets i.e., training, validation, and test. These tensor datasets were used to create so-called data loaders which turned the data into batches which were then passed through the NN. Here a couple of different batch sizes (64, 32 and 16) were tried out in order to improve the network.

For the network architecture itself, pytorch lightning was used. Different network architectures with an increasing number of layers and neurons were tried in order to improve the performance of the model. Six different architectures were examined further, starting from three layers and 100 neurons and ending up in seven layers with 1,024 artificial neurons. The linear or fully-connected layer of PyTorch was used for the models \citep{PyTorch2019, PyTorch2021}. Furthermore, different activation functions (SiLU, ELU and GELU) were applied to the model but the standard model architecture with rectified linear units (ReLUs) could not be improved with other activation functions.

An overwiew of the different network architectures that were tested and their respective performances is given in table \ref{tab:nn_pred}. The \enquote{neuron} column refers to the number of neurons in the first fully-connected layer (the number of neurons decreases with each layer). In the table, the validation performance refers to the F$_1$-Validation-Score in the last epoch. It can be seen that the F$_1$-Test-Score improved when the batch size was decreased while the number of layers, neurons, and epochs increased.