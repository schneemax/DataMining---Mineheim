\chapter{Data Mining}
%External Knowledge
%– Are additional datasets used?

% ML approaches
% – How many different ML approaches were tried out?
% – Do you have at least one symbolic and one non symbolic approach? (yes non-symbolic: decision tree; symbolic: NN)
% – Do you have at least one baseline (majority class / mean value / domain specific ...)? (yes: KNN is baseline)

% • Evaluation
% – Is there a train test split or 10-fold cross validation implemented
% – Is eval stratified?
% – Cost matrix or not?
% – Are the hyper parameters tuned (in which range / which attributes) ?
% – Are the tests systematic?
% – Analyse a symbolic model (how does the decision tre

%Additional Notes:
% Accuraccy raus

%Reihenfolge:
% 1. none (default)
% 2. 1. Hyperparameter tuning
% 3. +Balancing (Oversampling)
% 4. +Outliers
% 5. +Feature Selection
% 6. 2. Hyperparameter Tuning

After preprocessing, this part focuses on the variouse ML approaches that were used. In order to ensure the comparability of the ML approaches the \textit{K-Nearest Neighbors (K-NN)} method will be used as a baseline. As will be discussed in section \ref{sec:knn} the KNN approach is a rather simple, but potentially effective, classification method and for this reason suitable as a baseline. Other ML approaches that are investigated are \textit{Support Vector Machine (SVM)}, \textit{Descision Tree's}, its extension the \text{Random Forest} and \textit{Neural Networks}. This set up of variouse ML approaches was choosen for their coverage of different prediction strategies and in order to both cover symcolic and non-symbolic approaches.

The success of the different ML approaches is monitored through the Precision, Recall and F1-Makro metric for both train and test data. 
Precision and Recall both calculate the precision of a models predictions, with slight differences. Precision is an appropriate measurement to minimize false positives, while Recall an appropriate measurement is to minimize false negatives. Therefore Precision and Recall complement each other to reflect a meaningful statement of the models precision.\cite{Brownlee2020} F1-Makro on the other hand extends on these two. As a measurement that evaluates each class with the same importances, the F1-Score is helpful for unbalanced data sets like the one covered in this paper. The F1-Makro will expose models that only perform good on the common class whilste performing bad on rarer classes. For this reason and since the data set covered in this paper is unbalanced the F1-Makro will also be used to calculate the hyperparameter tuning.\cite{Peltarion2021}

The procedure to evaluate the different ML approaches begins with the training of the various models without any additional settings and the default values for each model. This first model, which will be referenced as \enquote*{none} in the result tables serves as a baseline. Afterwards, a hyperparameter tuning is performed. The hyperparameter tuning is evaluated through a ten fold cross validation. Based on this result the default values for the various models are changed. To further optimize potentially bad performing models, the data set is balanced through oversampling. Depending on the corresponding model a feature selection is conducted. Finally, a second hyperparameter tuning is carried out to optimize the altered ML model. Changes resulting from the various ML approaches are disclosed in the corresponding sections.


