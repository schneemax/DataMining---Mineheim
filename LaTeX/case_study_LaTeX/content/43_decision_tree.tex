\section{Decision Tree}\label{sec:decision_tree}
\begin{table}[htbp]
	\centering
	\begin{tabular}{
			>{\columncolor[HTML]{EFEFEF}}l |
			>{\columncolor[HTML]{FFFFFF}}l 
			>{\columncolor[HTML]{EFEFEF}}l |
			>{\columncolor[HTML]{FFFFFF}}l 
			>{\columncolor[HTML]{EFEFEF}}l |
			>{\columncolor[HTML]{FFFFFF}}l 
			>{\columncolor[HTML]{EFEFEF}}l |}
		\cline{2-7}
		\multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}}                            & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Precision}                                                 & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Recall}                                                    & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}F$_1$F1-Makro}                                                  \\ \hline
		\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Adaption}                   & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}Train} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Test} \\ \hline
		\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}none}                       & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.569                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.582                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}1.000} & 0.575                                             \\ \hline
		\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 1. Hp Tuning} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.822} & 0.484                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.714} & 0.46                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.757} & 0.468                                             \\ \hline
		\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Balancing}                & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.843} & 0.486                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.843} & 0.610                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.841} & 0.500                                             \\ \hline
		\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ Feature Selection}        & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.847} & 0.492                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.847} & 0.611                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.845} & 0.501                                             \\ \hline
		\multicolumn{1}{|l|}{\cellcolor[HTML]{EFEFEF}+ 2. Hp Tuning} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.869} & 0.485                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.866} & 0.600                                             & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}0.862} & 0.485                                             \\ \hline
	\end{tabular}
	\caption{Decision Tree - Predictions}
	\label{tab:rf_pred}
\end{table}

The decision tree is an algorithm with a tree-like structure which can be used for classification or regression tasks. The main idea is to build a tree out of nodes in which all attributes are tested and the best split is chosen. In the end, the child nodes should represent the corresponding class of a given sample. 

To start with, the first decision tree was created with the default parameters given by scikit-learn and was fitted to the train data. What can be immediately noticed is that all Precision, Recall, and the F$_1$-Macro score got a value of 1, whereas the model performed significantly worse on the test data. The resulting model predicted a medium quality most of the time. This overfitting with the test data was to be expected, since decision trees tend to overfit more easily. To address this, hyperparameter tuning was applied. To find better hyperparameters, a ten-fold cross-validation grid search was applied, where the parameters \textit{criterion, max\_depth, min\_samples\_split,} and \textit{min\_samples\_leaf} were iterated on. This led to a less overfitted model, although the test results did not really improve. To overcome the problem of only predicting medium wines, SMOTE oversampling was applied, as done earlier in the SVM. This improved the F$_1$-Score only marginally. Another improvement measure which was applied is to remove features from the data which do not seem to have a major impact on the model. By applying the in scikit-learn built-in feature importance methods, the feature \textit{type} was completely removed, since it had the lowest importance compared to other features. Nevertheless, this again did not seem to improve the model by a lot. In a last attempt, a second hyperparameter tuning with the same parameter grid was applied, resulting again in no further improvement on the test F$_1$-Score. In conclusion, it can be observed that a decision tree will not give the best classification results and should not be considered as a prediction model in this use case. Therefore, to improve the quality of the prediction, another classification algorithm such as the random forest algorithm should be evaluated.